{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d2cdca",
   "metadata": {},
   "source": [
    "# **DATA PREPROCESSING BASED ON EDA INSIGHTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a26f5",
   "metadata": {},
   "source": [
    "This notebook implements preprocessing steps based on the comprehensive EDA findings and recommendations. We'll follow the evidence-based approach from the EDA report to ensure our preprocessing aligns with the data patterns discovered.\n",
    "Based on the EDA report, we will:\n",
    "\n",
    "1. **Handle Skewed Variables** - Log-transform `ApplicantIncome` and `CoapplicantIncome`\n",
    "3. **Feature Engineering** - Create Total_income, Loan_to_income, and interaction features.\n",
    "4. **Encoding categorical columns**\n",
    "5. **Scaling** - RobustScaler \n",
    "6. **Target Handling** - Classification approach with stratified splits\n",
    "4. **Feature Selection** - Keep high-signal features, evaluate low-signal ones\n",
    "7. **Splitting into target and Features**\n",
    "\n",
    "\n",
    "**Key EDA Evidence to Implement**\n",
    "\n",
    "- **High-signal features**: `Credit_History, ApplicantIncome, CoapplicantIncome, LoanAmount, Property_Area`, and derived variables such as Total_Income and Loan_to_Income show strong or meaningful relationships with loan approval outcomes.\n",
    "- **Low-signal features**: `Gender, Married, Dependents, Self_Employed, and Loan_Amount_Term `display weak or negligible influence on loan approval and may be deprioritized during preprocessing.\n",
    "- **Skewed variables**: `ApplicantIncome` and `CoapplicantIncome` (log-transform)\n",
    "- **Feature engineering**: Total_Income\n",
    "Loan_to_Income, Loan_Term_Years, Has_Coapplicant, Dependents_Num.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7430d3",
   "metadata": {},
   "source": [
    "#### **1. Import Libraries and load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c889307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, skew\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('All libraries imported successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0ab008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "loan_train = pd.read_csv('home_loan_train.csv')\n",
    "\n",
    "loan_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85316fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataset for preprocessing\n",
    "df_processed = loan_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deb7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Loan_ID as the index \n",
    "df_processed.set_index('Loan_ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9056e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the Loan_Status from object to int to check correlation \n",
    "df_processed['Loan_Status'] = df_processed['Loan_Status'].map({'Y': 1, 'N': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5311051f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID\n",
       "LP001002    1\n",
       "LP001003    0\n",
       "LP001005    1\n",
       "LP001006    1\n",
       "LP001008    1\n",
       "           ..\n",
       "LP002978    1\n",
       "LP002979    1\n",
       "LP002983    1\n",
       "LP002984    1\n",
       "LP002990    0\n",
       "Name: Loan_Status, Length: 614, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display output\n",
    "df_processed['Loan_Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b42954",
   "metadata": {},
   "source": [
    "#### **2. EDA-Based Data Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ca2e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Missing Values:\n",
      "Gender              13\n",
      "Married              3\n",
      "Dependents          15\n",
      "Self_Employed       32\n",
      "LoanAmount          22\n",
      "Loan_Amount_Term    14\n",
      "Credit_History      50\n",
      "dtype: int64\n",
      "\n",
      "2. Duplicate Rows:\n",
      "Number of duplicated rows: 0\n",
      "\n",
      "3. Skewness Analysis (EDA identified right-skewed variables):\n",
      "ApplicantIncome: Skewness = 6.524 (right_skewed)\n",
      "CoapplicantIncome: Skewness = 7.473 (right_skewed)\n",
      "LoanAmount: Skewness = nan (Approx normal)\n",
      "\n",
      " 4. Correlation wit quality (EDA Evidence):\n",
      "High signal features (|Correlation| > 0.2)\n",
      "Credit_History: 0.562\n",
      "\n",
      " Low-signal features (|correlated| < 0.1)\n",
      "CoapplicantIncome: -0.059\n",
      "LoanAmount: -0.037\n",
      "Loan_Amount_Term: -0.021\n",
      "ApplicantIncome: -0.005\n"
     ]
    }
   ],
   "source": [
    "# 1. Check for missing values (EDA showed no missing values)\n",
    "print(\"\\n1. Missing Values:\")\n",
    "missing_values = df_processed.isna().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"No missing values found as expceted from the EDA\")\n",
    "\n",
    "\n",
    "# 2. Check for duplicates\n",
    "print(\"\\n2. Duplicate Rows:\")\n",
    "duplicates = df_processed.duplicated().sum()\n",
    "print(f\"Number of duplicated rows: {duplicates}\")\n",
    "if duplicates.sum() > 0:\n",
    "    print(f\"Percentage of duplicates: {(duplicates/len(df_processed))* 100:.2f}\")\n",
    "\n",
    "\n",
    "# 3. Check for skewness for variables identified in EDA as right-skewed\n",
    "print(f\"\\n3. Skewness Analysis (EDA identified right-skewed variables):\")\n",
    "skewed_var = ['ApplicantIncome','CoapplicantIncome','LoanAmount']\n",
    "for var in skewed_var:\n",
    "    if var in df_processed.columns:\n",
    "        skewness = skew(df_processed[var])\n",
    "        print(f\"{var}: Skewness = {skewness:.3f} ({'right_skewed' if skewness > 0.5 else 'Approx normal'})\")\n",
    "    \n",
    "\n",
    "# 4. Check the correlation with target (EDA evidence)\n",
    "print(\"\\n 4. Correlation wit quality (EDA Evidence):\")\n",
    "correlations = df_processed.select_dtypes(include=['number']).corr()['Loan_Status'].sort_values(key=abs, ascending=False)\n",
    "print(\"High signal features (|Correlation| > 0.2)\")\n",
    "high_signal = correlations[abs(correlations) > 0.2].drop('Loan_Status')\n",
    "for feature, corr in high_signal.items():\n",
    "    print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\n Low-signal features (|correlated| < 0.1)\")\n",
    "low_signal = correlations[abs(correlations) < 0.1]\n",
    "for feature, corr in low_signal.items():\n",
    "    print(f\"{feature}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1887de1",
   "metadata": {},
   "source": [
    "#### **3. Handle Missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297752d",
   "metadata": {},
   "source": [
    "**Handling missing data in the categorical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587e35a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID\n",
       "LP001002    good\n",
       "LP001003    good\n",
       "LP001005    good\n",
       "LP001006    good\n",
       "LP001008    good\n",
       "            ... \n",
       "LP002978    good\n",
       "LP002979    good\n",
       "LP002983    good\n",
       "LP002984    good\n",
       "LP002990     bad\n",
       "Name: Credit_History, Length: 614, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the Credit history to a string\n",
    "df_processed['Credit_History'] = df_processed['Credit_History'].astype(str)\n",
    "df_processed['Credit_History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94768700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the Credit history \n",
    "df_processed['Credit_History'] = df_processed['Credit_History'].map({'1.0': 'good', '0.0':'bad'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea993c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Loan_Amount_Term to a string\n",
    "df_processed['Loan_Amount_Term'] = df_processed['Loan_Amount_Term'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce1214f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n",
       "       'Loan_Amount_Term', 'Credit_History', 'Property_Area'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the categorical columns\n",
    "cat_col = df_processed.select_dtypes(include='object').columns\n",
    "cat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90abdb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender              0\n",
       "Married             0\n",
       "Dependents          0\n",
       "Education           0\n",
       "Self_Employed       0\n",
       "Loan_Amount_Term    0\n",
       "Credit_History      0\n",
       "Property_Area       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the mode of the categorical columns\n",
    "modes = df_processed[cat_col].apply(lambda x: x.value_counts().index[0])\n",
    "\n",
    "# Fill the the missing categorical dataset with the mode from each column\n",
    "df_processed[cat_col] = df_processed[cat_col].fillna(modes)\n",
    "\n",
    "# Check the missing values have been handles\n",
    "df_processed[cat_col].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5800e074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender                0\n",
       "Married               0\n",
       "Dependents            0\n",
       "Education             0\n",
       "Self_Employed         0\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount           22\n",
       "Loan_Amount_Term      0\n",
       "Credit_History        0\n",
       "Property_Area         0\n",
       "Loan_Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm cat_col missing values have been handled\n",
    "df_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7fc89",
   "metadata": {},
   "source": [
    "**Handling missing data in the numerical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb03774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Status'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the columns with numerical values\n",
    "num_col = df_processed.select_dtypes(include=['number']).columns\n",
    "num_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "144ac217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill the missing dataset in the ApplicantIncome with the value of the 3812.5\n",
      "Fill the missing dataset in the CoapplicantIncome with the value of the 1188.5\n",
      "Fill the missing dataset in the LoanAmount with the value of the 128.0\n",
      "Fill the missing dataset in the Loan_Status with the value of the 1.0\n"
     ]
    }
   ],
   "source": [
    "for col in num_col:\n",
    "    missing_num_col = df_processed[col].median()\n",
    "    df_processed[col].fillna(missing_num_col, inplace=True)\n",
    "    print(f\"Fill the missing dataset in the {col} with the value of the {missing_num_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8e7fad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender               0\n",
       "Married              0\n",
       "Dependents           0\n",
       "Education            0\n",
       "Self_Employed        0\n",
       "ApplicantIncome      0\n",
       "CoapplicantIncome    0\n",
       "LoanAmount           0\n",
       "Loan_Amount_Term     0\n",
       "Credit_History       0\n",
       "Property_Area        0\n",
       "Loan_Status          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm num_col missing values have been handled\n",
    "df_processed.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a567c2a",
   "metadata": {},
   "source": [
    "#### **4. Handle Duplicated values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5647a171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No duplicates to remove (as expected from EDA)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates if any (EDA didn't report duplivcates, but let's be thorough)\n",
    "if duplicates > 0:\n",
    "    print(f\"Removing {duplicates} duplicate rows...\")\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"Dataset shape after removing duplicates: {df_processed.shape}\")\n",
    "else:\n",
    "    print(\"✓ No duplicates to remove (as expected from EDA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08a7f2",
   "metadata": {},
   "source": [
    "#### **5. Log-Transform Skewed Variables (EDA Recommendation)**\n",
    "\n",
    "Based on EDA findings, transform the right-skewed variables identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba018da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOG-TRANSFORMING SKEWED VARIABLES ===\n",
      "EDA identified these variables as right-skewed and recommended log transforamtion:\n",
      "✓ ApplicantIncome: Applied log transformation\n",
      " Original Skewness: 6.524 -> Transformed skewness: 0.478\n",
      "✓ CoapplicantIncome: Applied log1p transformation (had 0.000 minimum value)\n",
      " Original Skewness: 7.473 -> Transformed skewness: -0.173\n",
      "✓ LoanAmount: Applied log transformation\n",
      " Original Skewness: 2.736 -> Transformed skewness: -0.195\n",
      "\n",
      " Dataset shape after log transformation: (614, 15)\n",
      "New log-tranformed columns: ['ApplicantIncome_log', 'CoapplicantIncome_log', 'LoanAmount_log']\n"
     ]
    }
   ],
   "source": [
    "# Log-transfrorm skewed varibaled as recommended by EDA\n",
    "print(\"=== LOG-TRANSFORMING SKEWED VARIABLES ===\")\n",
    "print(\"EDA identified these variables as right-skewed and recommended log transforamtion:\")\n",
    "\n",
    "# Variables to log-transform based on EDA findings\n",
    "skewed_var = ['ApplicantIncome','CoapplicantIncome','LoanAmount']\n",
    "for var in skewed_var:\n",
    "    if var in df_processed.columns:\n",
    "        # Checks if variable has zero or negative values\n",
    "        min_val = df_processed[var].min()\n",
    "        if min_val <=0:\n",
    "            # Use log1p for variables with zeros\n",
    "            df_processed[f'{var}_log'] = np.log1p(df_processed[var])\n",
    "            print(f'✓ {var}: Applied log1p transformation (had {min_val:.3f} minimum value)')\n",
    "\n",
    "        else:\n",
    "            # Use log for positive values only\n",
    "            df_processed[f'{var}_log'] = np.log(df_processed[var])\n",
    "            print(f\"✓ {var}: Applied log transformation\")\n",
    "\n",
    "        # Check for skewness before and after\n",
    "        original_skew = skew(df_processed[var])\n",
    "        transformed_skew = skew(df_processed[f'{var}_log'])\n",
    "        print(f\" Original Skewness: {original_skew:.3f} -> Transformed skewness: {transformed_skew:.3f}\")\n",
    "\n",
    "print(f\"\\n Dataset shape after log transformation: {df_processed.shape}\")\n",
    "print(\"New log-tranformed columns:\", [col for col in df_processed.columns if '_log' in col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b403fdc",
   "metadata": {},
   "source": [
    "#### **5. Outlier Treatment (EDA Recommendation)**\n",
    "\n",
    "Based on EDA findings, handle outliers using IQR-clipping method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment based on EDA recommendations\n",
    "print(\"----Outlier Treatment (IQR-clipping method)----\")\n",
    "print(\"EDA recommend IQR-clipping for extreme values to preserve data points\")\n",
    "\n",
    "# Define numerical columns (excluding target)\n",
    "num_col = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Loan_Status' in num_col:\n",
    "    num_col.remove('Loan_Status')\n",
    "\n",
    "print(f\"Treating ouliers in {len(num_col)} numerical features...\")\n",
    "\n",
    "# Apply IQR_clipping method\n",
    "outliers_clipped = 0\n",
    "for col in num_col:\n",
    "    Q1 = df_processed[col].quantile(0.25)\n",
    "    Q3 = df_processed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Countoutliers before clipping\n",
    "    outliers_before = ((df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)).sum()\n",
    "\n",
    "    if outliers_before > 0:\n",
    "        # Clip outliers\n",
    "        df_processed[col] = np.where(df_processed[col] < lower_bound, lower_bound, df_processed[col])\n",
    "        df_processed[col] = np.where(df_processed[col] > upper_bound, upper_bound, df_processed[col])\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
